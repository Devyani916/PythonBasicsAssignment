{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMeN3oLBYr5rQtUZ8gb749R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Devyani916/PythonBasicsAssignment/blob/main/Regression_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **REGRESSION ASSIGNMENT**"
      ],
      "metadata": {
        "id": "hd8cBNNFN-MA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Simple Linear Regression?\n",
        "- Simple linear regression is a statistical method used to model the relationship between one independent variable (predictor) and one dependent variable (response) with a straight line. It aims to find the line that best fits the data points, minimizing the difference between the observed values and the values predicted by the line.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "JGUu7AZHP8fT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What are the key assumptions of Simple Linear Regression?\n",
        "- The key assumptions of simple linear regression are linearity, independence, homoscedasticity, and normality of residuals.\n",
        "1. Linearity: The relationship between the independent variable (X) and the dependent variable (Y) is linear, meaning they can be represented by a straight line.\n",
        "2. Independence: The errors or residuals (the difference between the actual Y value and the predicted Y value) are independent of each other.\n",
        "3. Homoscedasticity: The variance of the errors is constant across all levels of the independent variable (X). This means the spread of the errors is consistent, not widening or narrowing as X changes.\n",
        "4. Normality of Residuals: The errors (residuals) are normally distributed.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "qxTx_yqqNqU-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What does the coefficient m represent in the equation Y= mX+c?\n",
        "- In the equation y = mx + c, the coefficient 'm' represents the slope or gradient of the line. It indicates how steep the line is and whether it's increasing (positive slope) or decreasing.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "baKkn3KfchSf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What does the intercept c represent in the equation Y= mX+c?\n",
        "- In the equation y = mx + c, the 'c' represents the y-intercept. This means it's the value of y when x is 0, or the point where the line crosses the y-axis on a graph. The y-intercept helps define the location of the line on a graph.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nCTBdyLEeSXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do we calculate the slope m in Simple Linear Regression?\n",
        "- The slope (m) in simple linear regression is calculated using the formula: m = (n * Σxy - Σx * Σy) / (n * Σx² - (Σx)²). This formula utilizes the sums of the independent (x) and dependent (y) variable values, their products (xy), and the squares of the independent variable values (x²).\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mhFj87uZenlr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
        "- The purpose of the least squares method in simple linear regression is to find the line of best fit that minimizes the sum of the squared differences between the observed data points and the corresponding points on the regression line.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "E0a7MYDtfNob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How is the coefficient of determination (R^2) interpreted in Simple Linear Regression?\n",
        "- The coefficient of determination (R²) is a number between 0 and 1 that measures how well a statistical model predicts an outcome. You can interpret the R² as the proportion of variation in the dependent variable that is predicted by the statistical model.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "QYkokH8Tfpdc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the Multiple Linear Regression?\n",
        "- Multiple linear regression is a statistical technique used to model the relationship between a dependent variable and two or more independent variables. It's an extension of simple linear regression, which only considers one independent variable.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "C3bxvn9DgGyd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is the main difference between Simple and Multiple Linear Regression?\n",
        "- The primary distinction between simple and multiple linear regression lies in the number of independent variables used to predict a dependent variable. Simple linear regression models the relationship between one independent variable and one dependent variable, while multiple linear regression utilizes two or more independent variables to predict a single dependent variable.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "L08DzemegXtg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What are the key assumptions of Multiple Linear Regression?\n",
        "- The key assumptions of multiple linear regression include a linear relationship between the variables, independence of errors, homoscedasticity, and normally distributed residuals. Additionally, there should be no multicollinearity among the independent variables.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "gEn_Vtr2gr1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression Model?\n",
        "- Heteroscedasticity in a Multiple Linear Regression Model refers to a situation where the variance (spread) of the error terms (residuals) is not constant across all levels of the independent variables.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "AnoQaoRsg_KN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "- In summary, addressing multicollinearity in a multiple linear regression model involves a combination of variable selection, regularization, data augmentation, and dimensionality reduction techniques. The specific approach will depend on the nature of the data, the model's objectives, and the extent of multicollinearity present in the model.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tOuGVS2DheCN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What are some common techniques for transforming categorical variables for use in regression?\n",
        "- There are several techniques to transform categorical variables for use in regression models. One-hot encoding creates binary variables for each category, while label encoding assigns numerical values to categories, useful for ordinal variables.\n",
        "- Other options include dummy coding, binning, and handling multicollinearity using regularization.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "EuaysBJOiIjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the role of interaction terms in Multiple Linear Regression?\n",
        "- In multiple linear regression, interaction terms represent the joint effect of two or more independent variables on the dependent variable, indicating that the relationship between one variable and the outcome changes depending on the level of another variable\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4LaiKfs5i5CD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How can the interpretation of intecept differ between Simple and Multiple Linear Regression?\n",
        "- In simple linear regression, the intercept represents the predicted value of the dependent variable when the independent variable is zero.\n",
        "- Whereas, in multiple linear regression, the intercept is the predicted value of the dependent variable when all independent variables are zero.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "y1qTotM3i5OB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "- In regression analysis, the slope represents the average change in the dependent variable for each unit change in the independent variable. It's crucial for understanding the relationship between variables and making accurate predictions. A steeper slope indicates a stronger relationship, while a flat slope suggests a weaker one.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "OyPdTCG2jtfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
        "- The intercept or constant in the regression model represents the mean value of the response variable when all the predictor variables in the model are equal to zero.\n",
        "- In linear regression, the intercept is the value of the dependent variable, i.e., Y when all values are independent variables, and Xs are zero.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "4xWZBtDKkNtK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. What are the limitations of using R^2 as a sole measure of model performance?\n",
        "- R^2 has several limitations while using as a sole measure of model performance that are, it doesn't indicate model quality, predictive error, or the presence of bias, and can be misleading in various situations.\n",
        "- It's also sensitive to model complexity, potentially inflating R-squared with irrelevant variables, and might not be suitable for non-linear models.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "UuU6XR9xIUVf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How would you interpret a large standard error for a regression coefficient?\n",
        "- A large standard error for a regression coefficient suggests that the estimated coefficient is less precise and more variable compared to a smaller standard error. This means there's a higher chance the true population value of the coefficient could be quite different from the estimated value. It indicates a less reliable estimate, possibly due to factors like multicollinearity or a small sample size.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "kUIxN39XJyjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. How can heteroscedasticity be identified in a residual plots, and why it is important to address it?\n",
        "- Heteroscedasticity, or unequal variance of errors, is visually identified in residual plots as a fan or cone-shaped pattern where the spread of residuals increases or decreases systematically with the fitted values.\n",
        "- It's important to address because it can lead to unreliable statistical inferences and inflate the variance of regression coefficient estimates.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "mXiLH4O7KP6c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What does it mean if a Multiple Linear Regression model has a high R^2 but low adjusted R^2?\n",
        "- A high R-squared and low adjusted R-squared in a multiple linear regression model suggest that the model includes irrelevant or highly correlated predictor variables, potentially leading to overfitting.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "-eMbCF-kLDKv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Why is it important to scale variables in Multiple Linear Regression?\n",
        "- Scaling variables in multiple linear regression is crucial for several reasons, primarily to improve model performance, stability, and interpretability. It helps prevent features with larger scales from dominating the regression process, ensures more efficient optimization, and enables easier comparison of coefficients, which are estimates of the effect of each variable on the dependent variable.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "eL_X63HALwbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. What is polynomial regression?\n",
        "- Polynomial regression is a form of regression analysis where the relationship between a dependent variable and one or more independent variables is modeled as an nth-degree polynomial. It's an extension of linear regression used when the relationship between variables is not linear.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "cPQeMsGWMcGk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. How does polynomial regression differ from linear regression?\n",
        "- Polynomial regression differs from linear regression in how it models the relationship between independent and dependent variables.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "DsHpYCGnMpfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. When is polynomial regression is used?\n",
        "- The goal of polynomial regression is to model a non-linear relationship between the independent and dependent variables. So that, Polynomial regression is used when the relationship between independent and dependent variables is not linear, and a curve is needed to model the data accurately.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "wKG_ens7NBwF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. What is the general equation for polynomial regression?\n",
        "- The general equation for polynomial regression is a function that models the relationship between a dependent variable (y) and an independent variable (x) using a polynomial of degree n.\n",
        "- It's expressed as:\n",
        "\n",
        "              `y = β₀ + β₁x + β₂x² + ... + βₙxⁿ + ε`\n",
        "\n",
        " where β₀, β₁, ..., βₙ are coefficients and ε represents the error term.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "oNhEbP8qNXj2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Can polynomial regression be applied to multiple regression?\n",
        "- Yes, polynomial regression can be applied in the context of multiple regression.Transitioning from simple linear regression to multiple regression and then polynomial regression in R can be done step by step. First generate sample data with a simple linear relationship between x and y, and a quadratic relationship between x and z.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Wo3cd5QROIti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. What are the limitations of polynomial regression?\n",
        "- Polynomial regression, while versatile, has limitations, primarily including the risk of overfitting, especially with higher-degree polynomials, and computational complexity as the degree increases. It also struggles with selecting the optimal degree and is more sensitive to outliers compared to linear regression.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "SDX0ylqTOtv5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "-Several methods can be used to evaluate model fit when selecting the degree of a polynomial in regression, including visual inspection of data, cross-validation, and information criteria like AIC and BIC.\n",
        "- Additionally, techniques like Analysis of Variance (ANOVA) can be used to assess if adding polynomial terms significantly improves the fit.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "79yypcPvO_Fy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Why is visualization important in polynomial regression?\n",
        "- Visualization is crucial in polynomial regression for understanding the relationship between variables and for assessing the model's fit. It allows for a clear understanding of the data's curvature, helps determine the appropriate polynomial degree, and facilitates the detection of potential overfitting or underfitting.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "d04joc9zPjrW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. How is polynomial regression implemented in Python?\n",
        "- Polynomial regression, implemented in Python, models non-linear relationships by adding polynomial terms to a linear regression. The scikit-learn library simplifies this process. Here's how it works:\n",
        "\n",
        " * Import Libraries: Import necessary libraries, including numpy for numerical operations, matplotlib for plotting, PolynomialFeatures for generating polynomial terms, and LinearRegression for fitting the model.\n",
        " * Generate or Load Data: Create or load the dataset, ensuring it's suitable for polynomial regression (i.e., exhibiting a non-linear relationship between variables).\n",
        " * Create Polynomial Features: Use PolynomialFeatures to transform the original features into polynomial features. Specify the degree of the polynomial, for example, degree=2 for a quadratic relationship.\n",
        " * Fit the Model: Apply linear regression to the polynomial features using LinearRegression. This step finds the coefficients that best fit the transformed data.\n",
        " * Make Predictions: Use the trained model to predict new values based on the polynomial features.\n",
        " * Visualize Results: Plot the original data points and the fitted polynomial regression curve to assess the model's performance.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "oYBXNu2kQJfA"
      }
    }
  ]
}